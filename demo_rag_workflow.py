import os
import sys
import asyncio
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
import warnings

# --- 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ---
from dotenv import load_dotenv
from openai import OpenAI, AsyncOpenAI
from pinecone import Pinecone
from opentelemetry import trace

# Issue 3 Fix: Arize/Phoenixì˜ ê³µì‹ ë“±ë¡ í•¨ìˆ˜ ì‚¬ìš©
try:
    from arize.otel import register as register_arize
except ImportError:
    print("âŒ 'arize-otel' íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pip install arize-otel")
    sys.exit(1)

# RAGAS
from ragas import evaluate, EvaluationDataset, SingleTurnSample
# Use ragas.metrics.collections for v0.4
from ragas.metrics.collections import Faithfulness, AnswerRelevancy, ContextPrecision
from ragas.llms import llm_factory
from ragas.embeddings import embedding_factory

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import json
from src.services.case_service import get_case_by_id

# ì½˜ì†” ì¶œë ¥ìš© ìƒ‰ìƒ
class Colors:
    HEADER = '\033[95m'
    BLUE = '\033[94m'
    GREEN = '\033[92m'
    CYAN = '\033[36m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- ì„¤ì •ê°’ ê²€ì¦ ---
REQUIRED_KEYS = [
    "OPENAI_API_KEY", "PINECONE_API_KEY", "PINECONE_INDEX_NAME",
    "SENTENCE_PINECONE_INDEX_NAME", "PHOENIX_SPACE_ID", "PHOENIX_API_KEY", "LANGSMITH_PROJECT"
]

def check_env():
    missing = [key for key in REQUIRED_KEYS if not os.getenv(key)]
    if missing:
        print(f"{Colors.FAIL}âŒ í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing)}{Colors.ENDC}")
        sys.exit(1)
    print(f"{Colors.GREEN}âœ… í™˜ê²½ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ{Colors.ENDC}")

# --- 2. Arize (Phoenix) Tracer ì„¤ì • (ê°œì„ ) ---
def setup_arize_tracer():
    """Arize Phoenixì˜ ê³µì‹ register í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Tracerë¥¼ ì•ˆì „í•˜ê²Œ ì„¤ì •"""
    print(f"\n{Colors.BLUE}ğŸ“¡ [Observability] Arize Phoenix ì„¤ì • ì¤‘...{Colors.ENDC}")
    try:
        register_arize(
            space_id=os.getenv("PHOENIX_SPACE_ID"),
            api_key=os.getenv("PHOENIX_API_KEY"),
            project_name=os.getenv("LANGSMITH_PROJECT") # LangSmithì™€ í”„ë¡œì íŠ¸ëª… í†µì¼
        )
        print(f"{Colors.GREEN}   -> Arize Phoenix ì—°ê²° ì„±ê³µ!{Colors.ENDC}")
        return trace.get_tracer("demo_rag_workflow")
    except Exception as e:
        print(f"{Colors.FAIL}   -> Arize Phoenix ì—°ê²° ì‹¤íŒ¨: {e}{Colors.ENDC}")
        return None

# --- 3. Pinecone ê²€ìƒ‰ (RAG) ---
def search_pinecone_index(
    tracer, pc: Pinecone, index_name: str, namespace: str, query_vector: List[float], 
    top_k: int = 3, filter_dict: Optional[Dict] = None, category: str = "General"
) -> List[str]:
    print(f"   -> [{category}] Index '{index_name}' (NS: {namespace}) ê²€ìƒ‰ ì¤‘...")
    if filter_dict:
        print(f"      Filter: {filter_dict}")
    
    with tracer.start_as_current_span(f"pinecone_search_{category}") as span:
        span.set_attribute("pinecone.index", index_name)
        span.set_attribute("pinecone.namespace", namespace)
        span.set_attribute("pinecone.top_k", top_k)
        if filter_dict:
            span.set_attribute("pinecone.filter", str(filter_dict))
            
        try:
            index = pc.Index(index_name)
            results = index.query(
                vector=query_vector, namespace=namespace, filter=filter_dict,
                top_k=top_k, include_metadata=True
            )
            contexts = []
            for match in results['matches']:
                meta = match['metadata']
                text = meta.get('text') or meta.get('raw_text') or str(meta)
                score = match['score']
                contexts.append(f"[{category}] {text}")
                print(f"      - {category} ë¬¸ì„œ(Score {score:.4f}): {text[:50]}...")
            
            span.set_attribute("pinecone.result_count", len(contexts))
            if not contexts:
                print(f"      âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤ (Score < 0.6 or No matches).")
            return contexts
        except Exception as e:
            print(f"{Colors.FAIL}      âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}{Colors.ENDC}")
            span.record_exception(e)
            return []

def perform_dual_search(tracer, query: str, crime_code: Optional[str] = None) -> List[str]:
    print(f"\n{Colors.BLUE}ğŸ” [RAG] ë“€ì–¼ ë²¡í„° ê²€ìƒ‰ ì‹œì‘ (ë²•ë ¹ + ì–‘í˜•ê¸°ì¤€)...{Colors.ENDC}")
    
    with tracer.start_as_current_span("Step 1: Retrieval (Dual Search)") as span:
        current_trace_id = span.get_span_context().trace_id
        print(f"   -> [Trace ID: {current_trace_id:x}] Retrieval Step Started")
        
        # Standard Attributes for Input/Output Tab
        span.set_attribute("input.value", query)
        
        # Custom Attributes
        span.set_attribute("rag.query", query)
        
        openai_client = OpenAI()
        print(f"   -> ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±: '{query}'")
        embedding_resp = openai_client.embeddings.create(input=query, model="text-embedding-3-small")
        query_vector = embedding_resp.data[0].embedding
        
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        all_contexts = []
        
        law_index = os.getenv("PINECONE_INDEX_NAME") or "bupblessyou-judgments"
        law_namespace = os.getenv("LAW_PINECONE_NAMESPACE") or "law_statue_criminal"
        
        # Pass tracer to search_pinecone_index
        laws = search_pinecone_index(
            tracer, pc, index_name=law_index, namespace=law_namespace, query_vector=query_vector,
            top_k=2, category="ë²•ë ¹", filter_dict=None
        )
        all_contexts.extend(laws)
        
        if crime_code:
            sentence_index = os.getenv("SENTENCE_PINECONE_INDEX_NAME") or "bupblessyou-sentence-v1"
            sentence_namespace = os.getenv("SENTENCE_PINECONE_NAMESPACE") or "sentence_criteria"
            guidelines = search_pinecone_index(
                tracer, pc, index_name=sentence_index, namespace=sentence_namespace, query_vector=query_vector,
                top_k=2, filter_dict={"crime_number": crime_code}, category="ì–‘í˜•ê¸°ì¤€"
            )
            all_contexts.extend(guidelines)
            
        print(f"   -> ì´ {len(all_contexts)}ê°œ ìœ íš¨ ë¬¸ì„œ ë°œê²¬")
        span.set_attribute("rag.total_documents", len(all_contexts))
        
        # Standard Attribute for Output (Convert list to string representation)
        import json
        span.set_attribute("output.value", json.dumps(all_contexts, ensure_ascii=False))
        
        return all_contexts

# --- 4. LLM ë‹µë³€ ìƒì„± ---
def generate_answer(tracer, query: str, contexts: List[str]) -> str:
    print(f"\n{Colors.BLUE}ğŸ¤– [LLM] ë‹µë³€ ìƒì„± ì¤‘...{Colors.ENDC}")
    
    with tracer.start_as_current_span("Step 2: LLM Generation") as span:
        current_trace_id = span.get_span_context().trace_id
        print(f"   -> [Trace ID: {current_trace_id:x}] Generation Step Started")
        
        client = OpenAI()
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        context_text = "\n\n".join(contexts)
        system_prompt = "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë²•ë¥  ì¡°ë ¥ìì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ë²•ë ¹]ì™€ [ì–‘í˜•ê¸°ì¤€] ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ ë…¼ë¦¬ì ì´ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. ì¶œì²˜(ë²•ë ¹, ì–‘í˜•ê¸°ì¤€)ë¥¼ ëª…ì‹œí•˜ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤."
        user_prompt = f"ì§ˆë¬¸: {query}\n\n[ì°¸ê³  ìë£Œ]\n{context_text}"
        
        # Standard Attributes for Input
        span.set_attribute("input.value", user_prompt)
        import json
        span.set_attribute("llm.input_messages", json.dumps([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ], ensure_ascii=False))

        # Custom Attributes
        span.set_attribute("llm.system_prompt", system_prompt)
        span.set_attribute("llm.user_prompt", user_prompt)
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            temperature=0,
            max_tokens=4000 # Increase output limit to avoid truncation
        )
        answer = response.choices[0].message.content
        print(f"   -> ìƒì„±ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:\n{Colors.CYAN}{answer[:150]}...{Colors.ENDC}")
        
        # Standard Attribute for Output
        span.set_attribute("output.value", answer)
        span.set_attribute("llm.response", answer)
        
        return answer

# --- 5. RAGAS í‰ê°€ ë° Arize ì „ì†¡ ---
async def evaluate_and_log(tracer, query: str, answer: str, contexts: List[str], scenario_name: str, reference: Optional[str] = None):
    print(f"\n{Colors.WARNING}âš–ï¸ [Evaluation] '{scenario_name}' RAGAS í‰ê°€ ì‹œì‘...{Colors.ENDC}")
    if not contexts:
        print(f"{Colors.FAIL}âŒ ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì–´ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.{Colors.ENDC}")
        return
    if not tracer:
        print(f"{Colors.FAIL}âŒ Tracerê°€ ì—†ì–´ í‰ê°€ ë° ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.{Colors.ENDC}")
        return

    # Create Ragas LLM and Embeddings using factory (requires AsyncOpenAI client for async execution)
    eval_openai_client = AsyncOpenAI()
    ragas_llm = llm_factory(model="gpt-4o-mini", client=eval_openai_client)
    ragas_embeddings = embedding_factory(model="text-embedding-3-small", client=eval_openai_client)
    
    # Instantiate metrics with Ragas LLM
    f_metric = Faithfulness(llm=ragas_llm)
    ar_metric = AnswerRelevancy(llm=ragas_llm, embeddings=ragas_embeddings)
    cp_metric = ContextPrecision(llm=ragas_llm)
    
    with tracer.start_as_current_span(f"Step 3: RAGAS Evaluation") as span:
        current_trace_id = span.get_span_context().trace_id
        print(f"   -> [Trace ID: {current_trace_id:x}] Evaluation Step Started")
        
        start_time = time.time()
        
        # Run manual evaluation using ascore (async)
        try:
            # Faithfulness
            f_result = await f_metric.ascore(
                user_input=query,
                response=answer,
                retrieved_contexts=contexts
            )
            faith_score = f_result.value
            
            # Answer Relevancy
            ar_result = await ar_metric.ascore(
                user_input=query,
                response=answer
            )
            relevancy_score = ar_result.value
            
            # Context Precision (Requires reference if strict, but let's try passing what we have)
            cp_score = 0.0
            if reference:
                cp_result = await cp_metric.ascore(
                    user_input=query,
                    retrieved_contexts=contexts,
                    reference=reference
                )
                cp_score = cp_result.value
            
            scores = {
                "faithfulness": faith_score,
                "answer_relevancy": relevancy_score,
                "context_precision": cp_score
            }
        except Exception as e:
            print(f"{Colors.FAIL}âš ï¸ í‰ê°€ ì‹¤íŒ¨: {e}{Colors.ENDC}")
            span.record_exception(e)
            scores = {}
            faith_score = 0.0
            relevancy_score = 0.0
            cp_score = 0.0

        duration = time.time() - start_time
        
        print(f"{Colors.GREEN}âœ… í‰ê°€ ì™„ë£Œ ({duration:.2f}ì´ˆ){Colors.ENDC}")
        print(f"   -> Faithfulness: {faith_score:.4f}")
        print(f"   -> Answer Relevancy: {relevancy_score:.4f}")
        print(f"   -> Context Precision: {cp_score:.4f}")
        
        span.set_attribute("ragas.faithfulness", faith_score)
        span.set_attribute("ragas.answer_relevancy", relevancy_score)
        span.set_attribute("ragas.context_precision", cp_score)
        span.set_attribute("rag.scenario", scenario_name)
        span.set_attribute("rag.question", query)
        span.set_attribute("rag.answer", answer)
        span.set_attribute("rag.scores", scores)
        span.set_attribute("rag.context_count", len(contexts))
        span.set_attribute("ragas.status", "success")
        
        print(f"\n{Colors.BLUE}ğŸ“¡ [Observability] Arize Trace ì „ì†¡ ì™„ë£Œ{Colors.ENDC}")
        print(f"   -> Span ID: {span.get_span_context().span_id:x}")
        print(f"   -> Trace ID: {span.get_span_context().trace_id:x}")

# --- ë©”ì¸ ì‹¤í–‰ íë¦„ ---
async def main():
    print(f"{Colors.BOLD}=================================================={Colors.ENDC}")
    print(f"{Colors.BOLD}   BupBlessYou Dual-Source RAG Demo Script        {Colors.ENDC}")
    print(f"{Colors.BOLD}=================================================={Colors.ENDC}")
    
    check_env()
    tracer = setup_arize_tracer()
    
    scenarios = [
        {
            "name": "ê°•ì œì¶”í–‰ (Indecent Act)",
            "query": "ì´ ì‚¬ê±´ì˜ ì²˜ë²Œ ë²•ê·œì™€ ì–‘í˜• ì¸ìëŠ” ë¬´ì—‡ì¸ê°€?",
            "code": "criterion_03",
            "case_id": 750,
            "reference": "ê°•ì œì¶”í–‰ì£„(í˜•ë²• ì œ298ì¡°)ëŠ” 10ë…„ ì´í•˜ì˜ ì§•ì—­ ë˜ëŠ” 1ì²œ500ë§Œì› ì´í•˜ì˜ ë²Œê¸ˆì— ì²˜í•©ë‹ˆë‹¤."
        }
    ]
    
    for scenario in scenarios:
        print(f"\n{Colors.HEADER}##################################################")
        print(f"# Scenario: {scenario['name']} (Filter: {scenario['code']})")
        print(f"##################################################{Colors.ENDC}")
        
        # Start a root span for the scenario
        with tracer.start_as_current_span(f"Workflow: {scenario['name']}") as root_span:
            root_trace_id = root_span.get_span_context().trace_id
            print(f"ğŸ“ [Root Trace ID: {root_trace_id:x}] Started workflow for '{scenario['name']}'")
            
            root_span.set_attribute("scenario.name", scenario['name'])
            root_span.set_attribute("scenario.code", scenario['code'])
            
            # Standard Attribute for Root Span Input
            root_span.set_attribute("input.value", scenario['query'])
            
            contexts = perform_dual_search(tracer, scenario['query'], scenario['code'])
            answer = generate_answer(tracer, scenario['query'], contexts)
            
            # Standard Attribute for Root Span Output
            root_span.set_attribute("output.value", answer)
            
            await evaluate_and_log(tracer, scenario['query'], answer, contexts, scenario['name'])
            
            print(f"ğŸ“ [Root Trace ID: {root_trace_id:x}] Workflow completed")
    
    print(f"\n{Colors.BOLD}ğŸ‰ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.{Colors.ENDC}")
    
    # í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì „ Trace ë°ì´í„° ê°•ì œ ì „ì†¡
    print(f"{Colors.BLUE}â³ Trace ë°ì´í„° ì „ì†¡ ì¤‘...{Colors.ENDC}")
    try:
        provider = trace.get_tracer_provider()
        if hasattr(provider, "force_flush"):
            provider.force_flush()
        elif hasattr(provider, "shutdown"):
            provider.shutdown()
        
        # ì•ˆì „ì„ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
        time.sleep(2)
        print(f"{Colors.GREEN}âœ… ì „ì†¡ ì™„ë£Œ ë° ì¢…ë£Œ{Colors.ENDC}")
    except Exception as e:
        print(f"{Colors.FAIL}âš ï¸ Trace ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}{Colors.ENDC}")

if __name__ == "__main__":
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        
    asyncio.run(main())